{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 171 Problem Set 2\n",
    "# Due Monday, October 28, 2019 @ 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read *all* cells carefully and answer all parts (both text and missing code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter your information below:\n",
    "\n",
    "<div style=\"color: #000000;background-color: #EEEEFF\">\n",
    "    Your Name (submitter): Ji Houn Huh<br>\n",
    "    \n",
    "<hr>\n",
    "\n",
    "Collaborators, optional (they do *not* need to submit their own)\n",
    "\n",
    "(max of 2 collaborators)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this problem set, we will revisit the same movie review data from last time.  We will first try to predict the numeric score of the review (regression) using ridge regression (problems 1 and 2).  Then, we will try to just predict whether it is a good review or bad review (problems 3 and 4).\n",
    "\n",
    "The code below imports all allowed libraries and loads the data.  The variables loaded are as follows\n",
    "- Training data:\n",
    "    - `trainX` the data matrix, as is standard.  Each feature is a little different from last time.  The ith feature corresponds to the ith most common word across all reviews.  It is still related to the number of times the word is used in the review.  However, this time instead of bucketing this number into a category, we use the real value. Except, that instead of the raw count, we record the number of standard deviations this raw count is away from the mean raw count.  Why?  Well, we will cover that in week 9 or 10.  However, this makes things work better.  So, if the value is 0, then this review uses this word the average number of times.  If the value is +1, this review uses this word one standard deviation higher than average.  If -1, it uses it one standard deviation less than average.\n",
    "    - `trainYreg` the regression prediciton values.  We don't predict the raw rating (from 0 to 10), but rather the difference of this raw rating and 5.  So if the value in this vector is +3, that means the rating was an 8.  If the value is -4, the actual rating was a 1.\n",
    "    - `trainYclass` the classification prediction values.  These are +1 for positive reviews and -1 for negative reviews, same as last time.\n",
    "- Testing data:\n",
    "    - `testX` same as `trainX` but for the testing set\n",
    "    - `testYreg` same as `trainYreg` but for the training set\n",
    "    - `testYclass` same as `trainYclass` but for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "# load the data\n",
    "def loadsparsedata(fn):\n",
    "    \n",
    "    fp = open(fn,\"r\")\n",
    "    lines = fp.readlines()\n",
    "    maxf = 0;\n",
    "    for line in lines:\n",
    "        for i in line.split()[1::2]:\n",
    "            maxf = max(maxf,int(i))\n",
    "    \n",
    "    X = np.zeros((len(lines),maxf))\n",
    "    Y = np.zeros((len(lines)))\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        values = line.split()\n",
    "        Y[i] = int(values[0])\n",
    "        for j,v in zip(values[1::2],values[2::2]):\n",
    "            X[i,int(j)-1] = int(v)\n",
    "    \n",
    "    X = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "    return X,Y\n",
    "\n",
    "def loadplusones(fn):\n",
    "    (X,Y) = loadsparsedata(fn)\n",
    "    X = np.column_stack((np.ones(X.shape[0]),X))\n",
    "    return X,Y\n",
    "\n",
    "(trainX,trainYreg) = loadplusones('sptrainreal.txt')\n",
    "(testX,testYreg) = loadplusones('sptestreal.txt')\n",
    "trainYreg = trainYreg - 5\n",
    "testYreg = testYreg - 5\n",
    "trainYclass = np.sign(trainYreg)\n",
    "testYclass = np.sign(testYreg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (trainYreg)\n",
    "#print(np.shape(trainYreg))\n",
    "#print(trainYreg.transpose())\n",
    "#print(np.shape(np.transpose(trainYreg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "    <font size=+2>Part I: Ridge Regression</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 1:</font> <font size=+1>(3 points)</font>\n",
    "    \n",
    "Complete the training and testing functions below for ridge regression.\n",
    "    \n",
    "Do **not** penalize the initial weight (corresponding to the intercept term).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnridge(X,Y,lam):\n",
    "    # X is the data matrix of shape (m,n)\n",
    "    # Y is are the target values of shape (m,)\n",
    "    # lam is the value of lambda (careful, lambda is a reserved keyword in python)\n",
    "    # function should return w of shape (n,)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Ax=b , you can call x = np.linalg.solve(A,b) which calculates the equivalent of x=A^(âˆ’1)b. \n",
    "    # numpy.linalg.inv() for inversing\n",
    "    #lengthX = X.len()\n",
    "    #w = 0;\n",
    "    #for i in range(1000):\n",
    "    #    for j in range(lengthX):\n",
    "            \n",
    "    #tranX = X.T\n",
    "    \n",
    "    # find matrix identity matrix\n",
    "    idenX = np.eye(trainX.shape[1])\n",
    "    idenX[0][0] = 0\n",
    "    w = np.linalg.solve((X.T@X + lam*(idenX)), X.T@Y)\n",
    "    #w = np.linalg.inv(tranX*X + lam*(idenX)) + tranX * Y\n",
    "    return w\n",
    "    \n",
    "def predictridge(X,w):\n",
    "    # X is the (testing) data of shape (m,n)\n",
    "    # w are the weights learned in ridge regression\n",
    "    # function should return Y, the predicted values of shape (m,)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #idenX = np.eye(trainX.shape[1])\n",
    "    #tranX = np.transpose(X)\n",
    "    Y = w@X.T\n",
    "    #Y = w / (np.linalg.inv(X.T@X)@tranX)\n",
    "    return Y\n",
    "\n",
    "def testridge(X,Y,w):\n",
    "    # X and Y are the testing data\n",
    "    # w are the weights from ridge regression\n",
    "    # returns the mean squared error\n",
    "    Ydelta = Y - predictridge(X,w)\n",
    "    return (Ydelta*Ydelta).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[[ 1.         -0.72846714 -0.78471274 ... -0.14621821 -0.1357387\n",
      "  -0.14266464]\n",
      " [ 1.         -0.12458302 -0.61232115 ... -0.14621821 -0.1357387\n",
      "  -0.14266464]\n",
      " [ 1.          0.13422446  0.24963682 ... -0.14621821 -0.1357387\n",
      "  -0.14266464]\n",
      " ...\n",
      " [ 1.          0.13422446 -0.26753796 ... -0.14621821 -0.1357387\n",
      "  -0.14266464]\n",
      " [ 1.         -0.46965966 -0.95710434 ... -0.14621821 -0.1357387\n",
      "  -0.14266464]\n",
      " [ 1.         -0.46965966 -0.61232115 ... -0.14621821 -0.1357387\n",
      "  -0.14266464]]\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX))\n",
    "print(trainX)\n",
    "#print(trainX)\n",
    "#np.shape(trainX)\n",
    "#print(np.shape(trainX))\n",
    "#print('\\ntestX')\n",
    "#print(testX)\n",
    "#print(np.shape(testX))\n",
    "#print (learnridge(trainX, trainY, 4))\n",
    "check=np.eye(3,k=0)\n",
    "check[0][0] = 0\n",
    "#print (check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 2:</font> <font size=+1>(5 points)</font>\n",
    "\n",
    "Use 3-fold cross validation to select the value of lambda for ridge regression, using `trainX` and `trainYreg`.  Plot the average across the three folds of the average squared error on the validation set as a function of lambda.  Use `plt.semilogx` for your plot (i.e., the horizontal axis for lambda should be on a log scale).  Use 10 values of lambda, arrange logarithmically evenly between $10^2$ and $10^5$.  See `np.logspace`.  If the number of data points does not divide by 3 evenly, just divide as evenly as possible.  `np.array_split` might help, but there are other ways.\n",
    "    \n",
    "Save the chosen value for lambda in `ridgelam`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_sec(X, Y, lam):\n",
    "    fold_sectionX = np.array_split(X,3)\n",
    "    fold_sectionY = np.array_split(Y,3)\n",
    "    # Initialize the array\n",
    "    averag = []\n",
    "    # 3 folds\n",
    "    for i in range(3):\n",
    "        initfoldx = fold_sectionX[:i]\n",
    "        initfoldy = fold_sectionY[:i]\n",
    "        # Get values that is not the validation set\n",
    "        if i < 2:\n",
    "            initfoldx += fold_sectionX[i+1:]\n",
    "            initfoldy += fold_sectionY[i+1:]\n",
    "        # Add to a vertial stack \n",
    "        fold_trainX = np.vstack(initfoldx)\n",
    "        #print(fold_trainX)\n",
    "        fold_trainY = np.hstack(initfoldy)\n",
    "        #print(fold_trainY)\n",
    "        fold_validateX = fold_sectionX[i]\n",
    "        fold_validateY = fold_sectionY[i]\n",
    "        #print(\"post validate\")\n",
    "        # Get the weight value\n",
    "        w = learnridge(fold_trainX, fold_trainY, lam)\n",
    "        # Add to list \n",
    "        averag.append(testridge(X, Y, w))\n",
    "        #print(\"appending\")\n",
    "        \n",
    "    #return the mean of the points\n",
    "    return np.mean(averag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# get the lambda value\n",
    "choices = np.logspace(2, 5,num=10)\n",
    "# Get the range of lamdas\n",
    "ase = []\n",
    "#for h in choices:\n",
    "ase = [fold_sec(trainX, trainYreg, h) for h in choices]\n",
    "# Set the ridgelam to the minimum of the lamdas\n",
    "ridgelam = min(ase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9611876048>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(choices,ase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "    <font size=+2>Part II: Logistic Regression</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function for logistic regression, $f(x) = \\sigma(x^\\top w)$, is\n",
    "\\begin{align*}\n",
    "l(y,\\hat{y}) &= -\\ln \\sigma(y\\hat{y}) \\\\\n",
    "\\text{and thus} \\\\\n",
    "L &= \\frac{1}{m}\\sum_{i=1}^m -\\ln \\sigma(y_i f(x_i))\n",
    "\\end{align*}\n",
    "In class, we derived that the resulting gradient was, therefore,\n",
    "\\begin{align*}\n",
    "\\nabla_w L &= \\frac{-1}{m}\\sum_{i=1}^m (1-p_i)y_ix_i \\\\\n",
    "\\text{where} \\\\\n",
    "p_i &= \\sigma(y_i w^\\top x_i)\n",
    "\\end{align*}\n",
    "And so the update rule for $w$ is\n",
    "\\begin{align*}\n",
    "w &\\leftarrow w + \\eta \\frac{1}{m}\\sum_{i=1}^m (1-p_i)y_ix_i \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 3:</font> <font size=+1>(3 points)</font>\n",
    "\n",
    "Modify the total loss function, $L$, to include a regularization term with strength $\\lambda/m$ that penalizes the sum of the squares of the weights.\n",
    "\n",
    "***Write the new loss function.  Derive the gradient descent rule for this new loss function.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Loss Function:\n",
    "\\begin{align*}\n",
    "L &= \\frac{1}{m}\\sum_{i=1}^m -\\ln \\sigma(y_i f(x_i)) + \\frac{\\lambda}{m}\\sum_{j=1}^n w^2_j\n",
    "\\end{align*}\n",
    "\n",
    "New Gradient Function:\n",
    "\\begin{align*}\n",
    "\\nabla_w L &= \\frac{-1}{m}\\sum_{i=1}^m (1-p_i)y_ix_i + \\frac{\\lambda}{m}\\sum_{j=1}^n 2w_j\\\\\n",
    "\\text{where} \\\\\n",
    "p_i &= \\sigma(y_i w^\\top x_i)\n",
    "\\end{align*}\n",
    "\n",
    "Updated rule for $w$:\n",
    "\\begin{align*}\n",
    "w &\\leftarrow w + \\eta (\\frac{-1}{m}\\sum_{i=1}^m (1-p_i)y_ix_i + \\frac{\\lambda}{m}\\sum_{j=1}^n 2w_j )\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR DERIVATION HERE (include the loss function, the new gradient, and the update rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 4:</font> <font size=+1>(7 points)</font>\n",
    "    \n",
    "Complete the training and testing functions below for logistic regression.  We will use a constant step size of 0.2.  Picking a good step size is tricky, but this one should work well for this assignment.  Start $w$ at 0.  Use **batch** (or standard) gradient descent.  Stochastic gradient descent is harder to tell whether it is converging.  Stop when the squared magnitude of the gradient vector is less that $10^{-5}$.  Do not penalize the initial weight, corresponding to the intercept term.\n",
    "    \n",
    "A few hints:\n",
    "- This function will need to be written without loops (except for the loop over iterations of gradient descent) to be fast enough for the next question.\n",
    "- You can use `print` to output debugging information (or even use pyplot to plot things!).  The line `clear_output(wait=True)` will clear the output, in case you don't want the cell's output to extend too far during debugging.  (please remove debugging output when submitting)\n",
    "- To check to see if it is working, you should look that the gradient is getting smaller, but (more importantly) that the objective function (the loss) is getting smaller.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnlogreg(X,Y,lam):\n",
    "    # X is the data matrix of shape (m,n)\n",
    "    # Y is are the target labels (+1,-1) of shape (m,)\n",
    "    # lam is the value of lambda (careful, lambda is a reserved keyword in python)\n",
    "    # function should return w of shape (n,)\n",
    "    \n",
    "    w=np.zeros((len(X[0])))\n",
    "    m=len(X)\n",
    "    n=len(X[0])\n",
    "    \n",
    "    # While either the starting magnitude is 0, or the number is greater or equal to 10^-5\n",
    "    while ((np.linalg.norm(w) == 0) or (np.linalg.norm(w)**2 >= .00001)):\n",
    "        g=np.zeros((len(X[1])))\n",
    "        it2=1\n",
    "        r1=(lam/m)*2*np.sum(w)    \n",
    "        for cheese in range(X.shape[0]):\n",
    "            a = Y[cheese] * X[cheese] * w.T\n",
    "            p1 = (1/(1 + np.exp(-a)))\n",
    "            g1 = (-(1-p1)*(Y[cheese]) * X[cheese])\n",
    "            g = g + ((-1/m)*g1)\n",
    "            \n",
    "            \n",
    "        #while it2 < m:\n",
    "        #    a=Y[it2]*w.T*X[it2]\n",
    "        #    a=a*(-1)\n",
    "        #    p1=(1/(1 + np.exp(a)))\n",
    "        #    g1=((1-p1)*(Y[it2]*X[it2]))\n",
    "        #    g1=g1*(-1)\n",
    "        #    g=g+((-1/m)*g1)\n",
    "        #    it2=it2+1\n",
    "        w=w-(.20*(g+r1))\n",
    "        #print(w)\n",
    "        #print(np.linalg.norm(w)**2)\n",
    "    \n",
    "    return w\n",
    "    \n",
    "def predictlogreg(X,w):\n",
    "    # X is the (testing) data of shape (m,n)\n",
    "    # w are the weights learned in ridge regression\n",
    "    # function should return Y, the predicted values of shape (m,) (all values either +1 or -1)\n",
    "    return 1/(1 + np.exp(-(w.T*X)))\n",
    "    \n",
    "def testlogreg(X,Y,w):\n",
    "    # X and Y are the testing data\n",
    "    # w are the weights from ridge regression\n",
    "    # returns the mean squared error\n",
    "    \n",
    "    Ypred = np.sign(predictlogreg(X,w)) ## should be +1/-1, but incase they are not\n",
    "    \n",
    "    return (Ypred!=np.sign(Y)).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 5:</font> <font size=+1>(4 points)</font>\n",
    "\n",
    "Use 3-fold cross validation to select the value of lambda for logistic regression, using `trainX` and `trainYclass`.  Plot the average across the three folds of the average squared error on the validation set as a function of lambda.  Use `plt.semilogx` for your plot (i.e., the horizontal axis for lambda should be on a log scale).  Use 10 values of lambda, arrange logarithmically evenly between $10^0$ and $10^4$.  See `np.logspace`. \n",
    "    \n",
    "Save the chosen value for lambda in `logreglam`\n",
    "    \n",
    "This part takes about 12 minutes, in my solutions.  This is a long time to wait for a debug cycle.  To debug your code, use fewer lambda values until you are sure your code is correct.  Better still, test learnlogreg separately until you are sure it is working.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreglam=0\n",
    "choices=np.logspace(2, 5, num=10)\n",
    "plt.semilogx(choices)\n",
    "\n",
    "lam1=500\n",
    "heck1=learnlogreg(trainX, trainYclass, lam1)\n",
    "heck2=testlogreg(trainX, trainYclass, heck1)\n",
    "print(heck2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_log(X, Y, lam):\n",
    "    fold_sectionlogX = np.array_split(X,3)\n",
    "    fold_sectionlogY = np.array_split(Y,3)\n",
    "    # Initialize the array\n",
    "    averaglog = []\n",
    "    # 3 folds\n",
    "    for i in range(3):\n",
    "        initfoldlogx = fold_sectionlogX[:i]\n",
    "        initfoldlogy = fold_sectionlogY[:i]\n",
    "        # Get values that is not the validation set\n",
    "        if i < 2:\n",
    "            initfoldlogx += fold_sectionlogX[i+1:]\n",
    "            initfoldlogy += fold_sectionlogY[i+1:]\n",
    "        # Add to a vertial stack \n",
    "        fold_trainlogX = np.vstack(initfoldlogx)\n",
    "        #print(fold_trainX)\n",
    "        fold_trainlogY = np.hstack(initfoldlogy)\n",
    "        #print(fold_trainY)\n",
    "        fold_validatelogX = fold_sectionlogX[i]\n",
    "        fold_validatelogY = fold_sectionlogY[i]\n",
    "        #print(\"post validate\")\n",
    "        # Get the weight value\n",
    "        w = learnlogreg(fold_trainlogX, fold_trainlogY, lam)\n",
    "        # Add to list \n",
    "        averaglog.append(testlogreg(X, Y, w))\n",
    "        #print(\"appending\")\n",
    "        \n",
    "    #return the mean of the points\n",
    "    return np.mean(averaglog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:18: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "zoink = np.logspace(0, 4,num=10)\n",
    "# Get the range of lamdas\n",
    "aselog = []\n",
    "#for h in choices:\n",
    "aselog = [fold_log(trainX, trainYclass, z) for z in zoink]\n",
    "# Set the ridgelam to the minimum of the lamdas\n",
    "logreglam = min(aselog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(zoink,aselog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "    <font size=+2>Part III: Testing</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "The code below retrains ridge regression and logistic regression using the fond values of lambda, above.  It then reports the average error for each on the **testing** data.\n",
    "    \n",
    "Perhaps more interestingly, it also reports the error rate if the ridge regression method is used to predict whether the review is positive.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wridge = learnridge(trainX,trainYreg,ridgelam)\n",
    "wlogreg = learnlogreg(trainX,trainYclass,logreglam)\n",
    "\n",
    "ridgemse = testridge(testX,testYreg,wridge)\n",
    "logregerrrate = testlogreg(testX,testYclass,wlogreg)\n",
    "ridgeerrrate = testlogreg(testX,testYclass,wridge)\n",
    "\n",
    "print(\"mean squared error for ridge rgression = %f\" % ridgemse)\n",
    "print(\"error rate for logistic regression = %f\" % logregerrrate)\n",
    "print(\"error rate for ridge regression = %f\" % ridgeerrrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 6:</font> <font size=+1>(3 points)</font>\n",
    "Given the results above, would you use ridge regression or logistic regression for this problem?  **Explain why.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would use ridgeregession for this problem. It appears that the error rate for the ridge regression is lower than the other option."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS 171 Python",
   "language": "python",
   "name": "cs171-cpu-limitation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
